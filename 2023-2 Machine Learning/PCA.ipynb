{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is PCA?\n",
    "* The PCA Algorithm works by computing principal components where it retains datasets in the **direction of maximum variance** in the original datasets\n",
    "* PCA is a procedure that uses an orthogonal transformation to convert a set of observations possibly correlated variables into a set of values of linearly uncorrelated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"activity_sleep_label.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is the step-by-step procedure of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Perform data pre-processing\n",
    "(e.g. scaling on D-dimensional data)\n",
    "* It is critical to perform normalization prior to implementing the PCA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude categorical data (dtype = string)\n",
    "to_be_normed = [col for col in data.columns if data[col].dtypes != 'object']\n",
    "# 1. Standardize the numerical data\n",
    "standardized_data = normalize(data[to_be_normed])\n",
    "standardized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Compute a covariance matrix\n",
    "* This is needed to understand how features are correlated with each other\n",
    "* The covariance matrix is N X N symmetric matrix where N is the # of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculate the covariance matrix\n",
    "cov_matrix = np.cov(standardized_data, rowvar=False)\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The covariance matrix is N X N symmetric matrix where N is the # of dimensions\n",
    "print(f\"Dimension of the data:{standardized_data.shape[1]}\")\n",
    "print(f\"Shape of the covariance matrix:{cov_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "* This is needed to determine the **principal components** of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate the eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "print(f\"Dimension of the data: {standardized_data.shape[1]}\")\n",
    "print(f\"The number of the eigenvectors: {len(eigenvectors)}\")\n",
    "print(f\"Shape of the eigenvectors: {eigenvectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Sort the eigenvalue in descending order and find the corresponding eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sort eigenvectors in descending order of eigenvalues\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalue = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:,sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_eigenvectors = eigenvectors[:,sorted_indices]\n",
    "sorted_eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Select the largest K cases (**K <= D**) and create a projection matrix (i.e. w) using the cases\n",
    "* K refers to the number of principal components\n",
    "* How to create a projection matrix?\n",
    "  * If K=2, select two corresponding eigenvectors identified by the sorted eigenvalues\n",
    "* How to determine the number of principal components?\n",
    "  * Scree plot: a line plot of the eigenvalues of the eigenvalues of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Select the largest 'num_components' eigenvectors\n",
    "num_components = 2\n",
    "principal_components = eigenvectors[:, 0:num_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Transform data into a lower-dimensional space using the projection matrix\n",
    "* For two principal components, we have:\n",
    "  * Transforming by **dot product** (normalized data, projection matrix)\n",
    "    * The dot product of the normalized data matrix and the projection matrix effectively projects the original data onto the subspace defined by the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Project the data onto the principal components\n",
    "transformed_data = np.dot(principal_components.transpose(), standardized_data.transpose()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Let's Compare classification w/w.o PCA (Preparation & functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Define a PCA function before the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, num_component=2, threshold = 0.01, scree = True):\n",
    "    # Exclude categorical data (dtype = string)\n",
    "    # to_be_normed = [col for col in X.columns if X[col].dtypes != 'object']\n",
    "    # 1. Standardize the numerical data\n",
    "    standardized_data = normalize(data)\n",
    "\n",
    "    # 2. Calculate the covariance matrix\n",
    "    cov_matrix = np.cov(standardized_data, rowvar=False)\n",
    "\n",
    "    # 3. Calculate the eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # 4. Sort eigenvectors in descending order of eigenvalues\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    sorted_eigenvalue = eigenvalues[sorted_indices]\n",
    "    sorted_eigenvectors = eigenvectors[:,sorted_indices]\n",
    "\n",
    "    # 5. Select the largest 'num_components' eigenvectors\n",
    "    principal_components = sorted_eigenvectors[:, 0:num_component]\n",
    "\n",
    "    # 6. Project the data onto the principal components\n",
    "    transformed_data = np.dot(principal_components.transpose(), standardized_data.transpose()).transpose()\n",
    "\n",
    "    # 7. Calculate the explained variance ratio\n",
    "    explained_variance_ratio = [(i/sum(eigenvalues)) for i in sorted_eigenvalue]\n",
    "\n",
    "    if scree == True:\n",
    "        scree_plot(explained_variance_ratio, num_component, threshold)\n",
    "    \n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. How would you determine the number of principal components?\n",
    "### Scree Plot\n",
    "* A scree plot is a line of the eigenvalues of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scree_plot(explained_variance_ratio, num_component, threshold):\n",
    "    num_components = len(explained_variance_ratio)\n",
    "\n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    cumulated = np.cumsum(explained_variance_ratio)\n",
    "    ax.bar(range(1, num_components +1), explained_variance_ratio, color = '#99ccff')\n",
    "    ax.plot(range(1, num_components+1), cumulated, color = 'black')\n",
    "    \n",
    "    max_val = 0\n",
    "    \n",
    "    for value in cumulated:\n",
    "        if (value - max_val) >= threshold:\n",
    "            max_val = value\n",
    "    \n",
    "    ax.vlines(num_component, 0, max(cumulated), color = \"blue\", linestyles=\"--\",label=\"your PC #\")\n",
    "    ax.vlines(np.where(cumulated==max_val)[0]+1, 0, max(cumulated), color = \"red\", linestyles=\"--\", label = \"Best PC #\")\n",
    "    \n",
    "    for i in range(num_components): \n",
    "        ax.annotate(r\"%s\" % ((str(explained_variance_ratio[i]*100)[:4])), (i+1, explained_variance_ratio[i]), va = \"bottom\", ha = \"center\")\n",
    "     \n",
    "    ax.set_xticks(np.arange(1,num_components+1))\n",
    "    ax.set_xlabel(\"Principal Components\", fontsize = 18)\n",
    "    ax.set_ylabel(\"Explained variance Ratio\", fontsize = 18)\n",
    "    \n",
    "    plt.legend(loc = \"best\", fontsize = 15)\n",
    "    plt.title('Scree plot - PCA Scratch', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,2:].to_numpy()\n",
    "y = data.iloc[:, 1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 세트를 학습 세트(training set)와 검증 세트(test set)로 나눔\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y,random_state=42)\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Data w/w.o. PCA\n",
    "### Without PCA (Just standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score 표준화: 평균을 0, 표준편차 1로 변환\n",
    "scaler = StandardScaler() # Scaler 객체 생성\n",
    "scaler.fit(X_train) # 스케일링(표준화)를 위한 평균과 표준 편차 계산\n",
    "X_train_stand = scaler.transform(X_train) # 스케일링(표준화 수행)\n",
    "X_test_stand = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With PCA (PC = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_2 = pca(X_train, 2, 0.01 , scree = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca_2 = pca(X_test, 2, 0.01, scree = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With PCA (PC = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_7 = pca(X_train, 7,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca_7 = pca(X_test, 7,0.01, scree = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Scratch vs library (scree plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Data\n",
    "X_train_norm = normalize(X_train)\n",
    "X_test_norm = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train_pca_lib = pca.fit_transform(X_train_norm)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lib_scree_plot(pca_instance, threshold = 0.01):\n",
    "    num_components = len(pca_instance.explained_variance_ratio_)\n",
    "    ind = np.arange(num_components)\n",
    "    vals = pca_instance.explained_variance_ratio_ \n",
    "    \n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    ax = fig.add_subplot()\n",
    "    cumvals = np.cumsum(vals)\n",
    "    \n",
    "    max_val = 0\n",
    "\n",
    "    for value in cumulated:\n",
    "        if (value - max_val) >= threshold:\n",
    "            max_val = value\n",
    "    \n",
    "    ax.vlines(pca.n_components_, 0, max(cumulated), color = \"blue\", linestyles=\"--\",label=\"your PC #\")\n",
    "    ax.vlines(np.where(cumulated==max_val)[0]+1, 0, max(cumulated), color = \"red\", linestyles=\"--\", label = \"Best PC #\")\n",
    "    \n",
    "    ax.bar(ind, vals, color = '#ff6f15') # Bar plot\n",
    "    ax.plot(ind, cumvals, color = 'black') # Line plot \n",
    "\n",
    "\n",
    "    for i in range(num_components): #라벨링(바 위에 텍스트(annotation) 쓰기)\n",
    "        ax.annotate(r\"%s\" % ((str(vals[i]*100)[:3])), (ind[i], vals[i]), va = \"bottom\", ha = \"center\", fontsize = 13)\n",
    "    \n",
    "    ax.set_xticks(np.arange(1,num_components+1))\n",
    "    ax.set_xlabel(\"Principal Components\", fontsize = 18)\n",
    "    ax.set_ylabel(\"Explained variance Ratio\", fontsize = 18)\n",
    "\n",
    "    plt.legend(loc = \"best\", fontsize = 15)\n",
    "    plt.title('Scree plot - sklearn.decomposition PCA', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_scree_plot(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. (K-NN classification) Determination of the optimal K-values of K-NN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_k(X_train, y_train, X_test, y_test, max_k = 30):\n",
    "    errors = []\n",
    "    for i in range(1, max_k+1):\n",
    "        knn = KNeighborsClassifier(n_neighbors = i)\n",
    "        knn.fit(X_train, y_train)\n",
    "        pred_i = knn.predict(X_test)\n",
    "        errors.append(np.mean(pred_i != y_test))\n",
    "\n",
    "    opt_k = errors.index(min(errors)) + 1\n",
    "    \n",
    "    plt.plot(range(1, max_k+1), errors, marker='o')\n",
    "    plt.vlines(opt_k, 0, max(errors), color = \"red\", linestyles=\"--\")\n",
    "    plt.title('Mean error depending on K-Value')\n",
    "    plt.xlabel('k-value')\n",
    "    plt.xticks(np.arange(1, max_k+1))\n",
    "    plt.ylabel('mean error')\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.show()\n",
    "    \n",
    "    return opt_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Plot the Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the performance of the model\n",
    "def confusion_plot(conf_matrix, clf, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=clf.classes_, yticklabels=clf.classes_)\n",
    "    plt.title(f'Confusion Matrix-{title}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy plot according to number of neighborhood (K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the list of the accuracy according to K-values for K-NN algorithm (for scratch PCA)\n",
    "def acc_k_val(X_train, y_train, X_test, y_test, max_k = 30):\n",
    "    accs = []\n",
    "    for i in range(1, max_k+1):\n",
    "        knn = KNeighborsClassifier(n_neighbors = i)\n",
    "        knn.fit(X_train, y_train)\n",
    "        pred_i = knn.predict(X_test)\n",
    "        acc = accuracy_score(y_test, pred_i)\n",
    "        accs.append(acc)\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the list of the accuracy according to K-values for K-NN algorithm (for library PCA)\n",
    "def acc_k_val_lib(X_train, y_train, X_test, y_test, n_comp ,max_k = 30):\n",
    "    accs = []\n",
    "    for i in range(1, max_k+1):\n",
    "        pca_lib = PCA(n_components = n_comp)\n",
    "        X_train_pca_lib = pca_lib.fit_transform(X_train)\n",
    "        X_test_pca_lib = pca_lib.fit_transform(X_test)\n",
    "        knn = KNeighborsClassifier(n_neighbors = i)\n",
    "        knn.fit(X_train_pca_lib, y_train)\n",
    "        pred_i = knn.predict(X_test_pca_lib)\n",
    "        acc = accuracy_score(y_test, pred_i)\n",
    "        accs.append(acc)\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Accuracy plot according to the number of components of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the list of accuracies according to the number of components\n",
    "def acc_component_num(X_train, y_train, X_test, y_test, k_num):\n",
    "    accs = []\n",
    "    max_num = X_train.shape[1]\n",
    "    for i in range(0, max_num+1):\n",
    "        if i == 0:\n",
    "            reduced_train = X_train\n",
    "            reduced_test = X_test\n",
    "        else:\n",
    "            reduced_train = pca(X_train, i, scree = False)\n",
    "            reduced_test = pca(X_test, i, scree=False)\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = k_num)\n",
    "        knn.fit(reduced_train, y_train)\n",
    "        pred_i = knn.predict(reduced_test)\n",
    "        acc = accuracy_score(y_test, pred_i)\n",
    "        accs.append(acc)\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. K-NN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1. Classification without PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pca를 쓰지 않기 때문에 standarzation을 따로 시켜줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k value for K-NN classifier\n",
    "opt_k = optimal_k(X_train_stand, y_train, X_test_stand, y_test, max_k = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN 분류기를 생성\n",
    "classifier = KNeighborsClassifier(n_neighbors=opt_k)\n",
    "# 분류기 학습\n",
    "classifier.fit(X_train_stand, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "y_pred_stand= classifier.predict(X_test_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Result of classification without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pca_conf_matrix= confusion_matrix(y_test, y_pred_stand)\n",
    "confusion_plot(no_pca_conf_matrix, classifier, \"without PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pca_performance = classification_report(y_test, y_pred_stand)\n",
    "print(no_pca_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Classification with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. When PC = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k value for K-NN classifier\n",
    "opt_k2 = optimal_k(X_train_pca_2, y_train, X_test_pca_2, y_test, max_k = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN 분류기를 생성\n",
    "classifier = KNeighborsClassifier(n_neighbors=opt_k2)\n",
    "# 분류기 학습\n",
    "classifier.fit(X_train_pca_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "y_pred_pca2= classifier.predict(X_test_pca_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2_conf_matrix= confusion_matrix(y_test, y_pred_pca2)\n",
    "confusion_plot(pca2_conf_matrix, classifier, \"with PCA(PC=2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2_performance = classification_report(y_test, y_pred_pca2)\n",
    "print(pca2_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. When PC = 6 (Best # of PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k value for K-NN classifier\n",
    "opt_k7 = optimal_k(X_train_pca_7, y_train, X_test_pca_7, y_test, max_k = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN 분류기를 생성\n",
    "classifier = KNeighborsClassifier(n_neighbors=opt_k7)\n",
    "# 분류기 학습\n",
    "classifier.fit(X_train_pca_7, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "y_pred_pca7= classifier.predict(X_test_pca_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca7_conf_matrix= confusion_matrix(y_test, y_pred_pca7)\n",
    "confusion_plot(pca7_conf_matrix, classifier, \"with PCA(PC = 7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca7_performance = classification_report(y_test, y_pred_pca7)\n",
    "print(pca7_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Same Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Accuracy of K-NN classification according to K values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "\n",
    "acc_no = acc_k_val(X_train_stand, y_train, X_test_stand, y_test, k)\n",
    "acc_2 = acc_k_val(X_train_pca_2, y_train, X_test_pca_2, y_test, k)\n",
    "acc_7 = acc_k_val(X_train_pca_7, y_train, X_test_pca_7, y_test, k)\n",
    "\n",
    "plt.plot(range(1, k+1), acc_no, marker='o', label = \"K-NN\")\n",
    "plt.plot(range(1, k+1), acc_2, marker='o', label = \"K-NN + PCA(2)\")\n",
    "plt.plot(range(1, k+1), acc_7, marker='o', label = \"K-NN + PCA(7)\")\n",
    "plt.title('Accuracy of K-NN depending on K-Value (Scratch PCA)')\n",
    "plt.xlabel('k-value')\n",
    "plt.xticks(np.arange(1, k+1))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.yticks(np.arange(0.20, 0.75, 0.05))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = normalize(X_train)\n",
    "X_test_norm = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "\n",
    "acc_2_lib = acc_k_val_lib(X_train_norm, y_train, X_test_norm, y_test, 2, k)\n",
    "acc_7_lib = acc_k_val_lib(X_train_norm, y_train, X_test_norm, y_test, 7, k)\n",
    "\n",
    "plt.plot(range(1, k+1), acc_no, marker='o', label = \"K-NN\")\n",
    "plt.plot(range(1, k+1), acc_2_lib, marker='o', label = \"K-NN + PCA(2)\")\n",
    "plt.plot(range(1, k+1), acc_7_lib, marker='o', label = \"K-NN + PCA(7)\")\n",
    "plt.title('Accuracy of K-NN depending on K-Value (sklearn.decomposition PCA)')\n",
    "plt.xlabel('k-value')\n",
    "plt.xticks(np.arange(1, k+1))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.yticks(np.arange(0.20, 0.75, 0.05))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Accuracy according to the number of Principal component (k = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_to_pca_6 = acc_component_num(X_train, y_train, X_test, y_test, 6)\n",
    "acc_to_pca_15 = acc_component_num(X_train, y_train, X_test, y_test, 15)\n",
    "acc_to_pca_22 = acc_component_num(X_train, y_train, X_test, y_test, 22)\n",
    "\n",
    "plt.figure(figsize=(25,6))\n",
    "\n",
    "plt.plot(range(0, X_train.shape[1]+1), acc_to_pca_6, marker='o', label = \"KNN\")\n",
    "plt.plot(range(0, X_train.shape[1]+1), acc_to_pca_15, marker='o', label = \"KNN+PCA(PC=2)\")\n",
    "plt.plot(range(0, X_train.shape[1]+1), acc_to_pca_22, marker='o', label = \"KNN+PCA(PC=7)\")\n",
    "plt.title(f'Accuracy of K-NN depending on number of components for PCA', fontsize=18)\n",
    "plt.xlabel('num of pca', fontsize = 15)\n",
    "plt.xticks(np.arange(0, X_train.shape[1]+1), fontsize = 12)\n",
    "plt.ylabel('Accuracy', fontsize = 15)\n",
    "\n",
    "plt.legend(loc = \"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. [PCA scratch](https://www.askpython.com/python/examples/principal-component-analysis)\n",
    "2. [Data Normalization](https://stackoverflow.com/questions/21030391/how-to-normalize-a-numpy-array-to-a-unit-vector)\n",
    "3. [Scree plot - concept](https://sanchitamangale12.medium.com/scree-plot-733ed72c8608)\n",
    "4. [Scree plot - implementation](https://velog.io/@yuns_u/PCA-Scree-Plot)\n",
    "5. [K-NN Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "6. [Debugging and visualization](https://chat.openai.com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
